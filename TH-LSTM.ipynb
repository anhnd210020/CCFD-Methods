{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] Loss: 0.0022 | Accuracy: 0.9995\n",
      "Threshold: 0.10 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.20 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.30 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.40 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.50 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.70 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.90 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert date to timestamp\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time']).apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.label_encoder.fit_transform(self.data['category'])\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, -2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# TH-LSTM Model (with time-aware gate `T_t`)\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wfs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wis = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wuh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wux = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wus = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.WTh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.WTx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.WTs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bT = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wos = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)\n",
    "\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            f_t = torch.sigmoid(self.Wfh(h_prev) + self.Wfx(x_t) + self.Wfs(s_t) + self.bf)\n",
    "            i_t = torch.sigmoid(self.Wih(h_prev) + self.Wix(x_t) + self.Wis(s_t) + self.bi)\n",
    "            T_t = torch.sigmoid(self.WTh(h_prev) + self.WTx(x_t) + self.WTs(s_t) + self.bT)\n",
    "            zeta_t = torch.tanh(self.Wuh(h_prev) + self.Wux(x_t) + self.Wus(s_t) + self.bu)\n",
    "            c_t = f_t * c_prev + i_t * zeta_t + T_t * s_t\n",
    "            o_t = torch.sigmoid(self.Woh(h_prev) + self.Wox(x_t) + self.Wos(s_t) + self.bo)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            historical_states = torch.cat((historical_states[:, 1:], h_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        q_t = torch.cat((h_t, c_t), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1)\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        y_pred = torch.sigmoid(self.classifier(e_t))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 3  # category, amt, delta_t\n",
    "    hidden_dim = 64\n",
    "    memory_size = 10\n",
    "    seq_len = 5\n",
    "    epochs = 1\n",
    "\n",
    "    train_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTrain.csv\", seq_len=seq_len)\n",
    "    test_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTest.csv\", seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TH_LSTM(input_dim, hidden_dim, memory_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        y_true_epoch = []\n",
    "        y_pred_epoch = []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_pred_binary = (y_pred >= 0.5).float()\n",
    "            y_true_epoch.extend(y_batch.cpu().numpy())\n",
    "            y_pred_epoch.extend(y_pred_binary.cpu().numpy())\n",
    "\n",
    "        epoch_accuracy = accuracy_score(y_true_epoch, y_pred_epoch)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {total_loss / len(train_loader):.4f} | Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # Testing phase with thresholds\n",
    "    model.eval()\n",
    "    y_true_test = []\n",
    "    y_pred_test_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            y_pred = model(X_batch)\n",
    "            y_true_test.extend(y_batch.cpu().numpy())\n",
    "            y_pred_test_prob.extend(y_pred.cpu().numpy())\n",
    "\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9]\n",
    "    for threshold in thresholds:\n",
    "        y_pred_test_binary = (np.array(y_pred_test_prob) >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_true_test, y_pred_test_binary)\n",
    "        precision = precision_score(y_true_test, y_pred_test_binary, zero_division=0)\n",
    "        recall = recall_score(y_true_test, y_pred_test_binary)\n",
    "        f1 = f1_score(y_true_test, y_pred_test_binary)\n",
    "        auc = roc_auc_score(y_true_test, y_pred_test_binary)\n",
    "\n",
    "        print(f\"Threshold: {threshold:.2f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng thẻ tín dụng xuất hiện trong cả tập train và test: 908\n"
     ]
    }
   ],
   "source": [
    "train_cards = set(train_dataset.data['cc_num'])\n",
    "test_cards = set(test_dataset.data['cc_num'])\n",
    "overlap = train_cards.intersection(test_cards)\n",
    "print(f\"Số lượng thẻ tín dụng xuất hiện trong cả tập train và test: {len(overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giao dịch còn lại trong tập kiểm tra: 163\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ các thẻ tín dụng trùng lặp\n",
    "test_dataset.data = test_dataset.data[~test_dataset.data['cc_num'].isin(train_cards)]\n",
    "print(f\"Số lượng giao dịch còn lại trong tập kiểm tra: {len(test_dataset.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giao dịch trong tập train mới: 1034175\n",
      "Số lượng giao dịch trong tập test mới: 262500\n",
      "Số thẻ tín dụng trong tập train mới: 786\n",
      "Số thẻ tín dụng trong tập test mới: 197\n"
     ]
    }
   ],
   "source": [
    "# Lấy danh sách tất cả các thẻ tín dụng từ tập train\n",
    "train_cc_nums = train_dataset.data['cc_num'].unique()  # Các thẻ tín dụng trong tập train ban đầu\n",
    "np.random.shuffle(train_cc_nums)  # Xáo trộn để đảm bảo ngẫu nhiên\n",
    "\n",
    "# Chia lại tập train thành hai tập mới\n",
    "new_train_cc_nums = train_cc_nums[:int(0.8 * len(train_cc_nums))]\n",
    "new_test_cc_nums = train_cc_nums[int(0.8 * len(train_cc_nums)):]\n",
    "\n",
    "# Tạo tập train và test mới\n",
    "new_train_data = train_dataset.data[train_dataset.data['cc_num'].isin(new_train_cc_nums)]\n",
    "new_test_data = train_dataset.data[train_dataset.data['cc_num'].isin(new_test_cc_nums)]\n",
    "\n",
    "print(f\"Số lượng giao dịch trong tập train mới: {len(new_train_data)}\")\n",
    "print(f\"Số lượng giao dịch trong tập test mới: {len(new_test_data)}\")\n",
    "print(f\"Số thẻ tín dụng trong tập train mới: {len(new_train_cc_nums)}\")\n",
    "print(f\"Số thẻ tín dụng trong tập test mới: {len(new_test_cc_nums)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still have Optimi but little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train size: 1032454 transactions\n",
      "New validation size: 264221 transactions\n",
      "Epoch [1/3] Loss: 0.0022 | Train Accuracy: 0.9995\n",
      "Validation Metrics:\n",
      "Threshold: 0.10 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.20 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.30 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.40 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.50 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Epoch [2/3] Loss: 0.0005 | Train Accuracy: 0.9999\n",
      "Validation Metrics:\n",
      "Threshold: 0.10 | Accuracy: 0.9999 | Precision: 0.9912 | Recall: 1.0000 | F1: 0.9956 | AUC: 1.0000\n",
      "Threshold: 0.20 | Accuracy: 1.0000 | Precision: 0.9931 | Recall: 1.0000 | F1: 0.9965 | AUC: 1.0000\n",
      "Threshold: 0.30 | Accuracy: 1.0000 | Precision: 0.9956 | Recall: 1.0000 | F1: 0.9978 | AUC: 1.0000\n",
      "Threshold: 0.40 | Accuracy: 1.0000 | Precision: 0.9969 | Recall: 1.0000 | F1: 0.9984 | AUC: 1.0000\n",
      "Threshold: 0.50 | Accuracy: 1.0000 | Precision: 0.9975 | Recall: 1.0000 | F1: 0.9987 | AUC: 1.0000\n",
      "Epoch [3/3] Loss: 0.0000 | Train Accuracy: 1.0000\n",
      "Validation Metrics:\n",
      "Threshold: 0.10 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.20 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.30 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.40 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n",
      "Threshold: 0.50 | Accuracy: 1.0000 | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000 | AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import random\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data.copy()\n",
    "\n",
    "        # Convert date to timestamp\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time']).apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.label_encoder.fit_transform(self.data['category'])\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, -2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# TH-LSTM Model (with time-aware gate `T_t`)\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wfs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wis = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wuh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wux = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wus = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.WTh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.WTx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.WTs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bT = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wos = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)\n",
    "\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            f_t = torch.sigmoid(self.Wfh(h_prev) + self.Wfx(x_t) + self.Wfs(s_t) + self.bf)\n",
    "            i_t = torch.sigmoid(self.Wih(h_prev) + self.Wix(x_t) + self.Wis(s_t) + self.bi)\n",
    "            T_t = torch.sigmoid(self.WTh(h_prev) + self.WTx(x_t) + self.WTs(s_t) + self.bT)\n",
    "            zeta_t = torch.tanh(self.Wuh(h_prev) + self.Wux(x_t) + self.Wus(s_t) + self.bu)\n",
    "            c_t = f_t * c_prev + i_t * zeta_t + T_t * s_t\n",
    "            o_t = torch.sigmoid(self.Woh(h_prev) + self.Wox(x_t) + self.Wos(s_t) + self.bo)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            historical_states = torch.cat((historical_states[:, 1:], h_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        q_t = torch.cat((h_t, c_t), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1)\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        y_pred = torch.sigmoid(self.classifier(e_t))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 3  # category, amt, delta_t\n",
    "    hidden_dim = 32\n",
    "    memory_size = 10\n",
    "    seq_len = 5\n",
    "    epochs = 3\n",
    "\n",
    "    # Load training dataset\n",
    "    train_data = pd.read_csv(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTrain.csv\")\n",
    "\n",
    "    # Split the training data into 80% for train and 20% for validation\n",
    "    all_cc_nums = train_data['cc_num'].unique()\n",
    "    random.shuffle(all_cc_nums)\n",
    "\n",
    "    train_cc_nums = all_cc_nums[:int(0.8 * len(all_cc_nums))]\n",
    "    val_cc_nums = all_cc_nums[int(0.8 * len(all_cc_nums)):]\n",
    "\n",
    "    new_train_data = train_data[train_data['cc_num'].isin(train_cc_nums)]\n",
    "    new_val_data = train_data[train_data['cc_num'].isin(val_cc_nums)]\n",
    "\n",
    "    print(f\"New train size: {len(new_train_data)} transactions\")\n",
    "    print(f\"New validation size: {len(new_val_data)} transactions\")\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = CreditCardFraudDataset(new_train_data, seq_len=seq_len)\n",
    "    val_dataset = CreditCardFraudDataset(new_val_data, seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define model\n",
    "    model = TH_LSTM(input_dim, hidden_dim, memory_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop with accuracy and validation metrics at different thresholds\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Collect predictions and labels for training accuracy\n",
    "        train_preds.extend(y_pred.detach().cpu().numpy())\n",
    "        train_labels.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "    train_preds = np.array(train_preds).flatten()\n",
    "    train_labels = np.array(train_labels).flatten()\n",
    "    train_pred_binary = (train_preds >= 0.5).astype(int)\n",
    "    train_accuracy = accuracy_score(train_labels, train_pred_binary)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {total_loss / len(train_loader):.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to('cuda'), y_val.view(-1, 1).to('cuda')\n",
    "            y_val_pred = model(X_val)\n",
    "            val_preds.extend(y_val_pred.cpu().numpy())\n",
    "            val_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "    val_preds = np.array(val_preds).flatten()\n",
    "    val_labels = np.array(val_labels).flatten()\n",
    "\n",
    "    # Calculate metrics for each threshold\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    print(\"Validation Metrics:\")\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresholded = (val_preds >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(val_labels, y_pred_thresholded)\n",
    "        precision = precision_score(val_labels, y_pred_thresholded, zero_division=0)\n",
    "        recall = recall_score(val_labels, y_pred_thresholded)\n",
    "        f1 = f1_score(val_labels, y_pred_thresholded)\n",
    "        auc = roc_auc_score(val_labels, val_preds)\n",
    "        \n",
    "        print(f\"Threshold: {threshold:.2f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect with 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] Loss: 0.0050\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import random\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data.copy()\n",
    "\n",
    "        # Convert date to timestamp\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time']).apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.label_encoder.fit_transform(self.data['category'])\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, -2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# TH-LSTM Model (with Dropout and Regularization)\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Dropout: ngẫu nhiên bỏ qua 30% nơ-ron\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wfs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wis = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wuh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wux = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wus = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.WTh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.WTx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.WTs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bT = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wos = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)\n",
    "\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            f_t = torch.sigmoid(self.Wfh(h_prev) + self.Wfx(x_t) + self.Wfs(s_t) + self.bf)\n",
    "            i_t = torch.sigmoid(self.Wih(h_prev) + self.Wix(x_t) + self.Wis(s_t) + self.bi)\n",
    "            T_t = torch.sigmoid(self.WTh(h_prev) + self.WTx(x_t) + self.WTs(s_t) + self.bT)\n",
    "            zeta_t = torch.tanh(self.Wuh(h_prev) + self.Wux(x_t) + self.Wus(s_t) + self.bu)\n",
    "            c_t = f_t * c_prev + i_t * zeta_t + T_t * s_t\n",
    "            o_t = torch.sigmoid(self.Woh(h_prev) + self.Wox(x_t) + self.Wos(s_t) + self.bo)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            # Add dropout at the hidden layer\n",
    "            h_t = self.dropout(h_t)  # Dropout at the output layer\n",
    "\n",
    "            historical_states = torch.cat((historical_states[:, 1:], h_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        q_t = torch.cat((h_t, c_t), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1)\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        y_pred = torch.sigmoid(self.classifier(e_t))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Training loop with L2 Regularization (weight_decay)\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 3  # category, amt, delta_t\n",
    "    hidden_dim = 8  # Đã giảm kích thước hidden_dim từ 64 xuống 32\n",
    "    memory_size = 10\n",
    "    seq_len = 5\n",
    "    epochs = 1\n",
    "\n",
    "    # Load dataset\n",
    "    train_data = pd.read_csv(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTrain.csv\")\n",
    "\n",
    "    # Split the training data (80% train, 20% validation)\n",
    "    all_cc_nums = train_data['cc_num'].unique()\n",
    "    random.shuffle(all_cc_nums)\n",
    "    train_cc_nums = all_cc_nums[:int(0.8 * len(all_cc_nums))]\n",
    "    val_cc_nums = all_cc_nums[int(0.8 * len(all_cc_nums)):]\n",
    "\n",
    "    new_train_data = train_data[train_data['cc_num'].isin(train_cc_nums)]\n",
    "    new_val_data = train_data[train_data['cc_num'].isin(val_cc_nums)]\n",
    "\n",
    "    train_dataset = CreditCardFraudDataset(new_train_data, seq_len=seq_len)\n",
    "    val_dataset = CreditCardFraudDataset(new_val_data, seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define model, optimizer, and loss function with weight decay (L2 Regularization)\n",
    "    model = TH_LSTM(input_dim, hidden_dim, memory_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Validation Loss: 0.0003\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9986\n",
      "F1-Score: 0.9993\n",
      "AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred_prob = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.extend(y_batch.cpu().numpy())  # True labels\n",
    "            y_pred_prob.extend(y_pred.cpu().numpy())  # Predicted probabilities\n",
    "\n",
    "    # Convert probabilities to binary predictions with threshold 0.5\n",
    "    y_pred_binary = (np.array(y_pred_prob) >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary)\n",
    "    f1 = f1_score(y_true, y_pred_binary)\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n",
    "\n",
    "# After training, evaluate the model on the validation set\n",
    "val_loss, val_accuracy, val_precision, val_recall, val_f1, val_auc = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1-Score: {val_f1:.4f}\")\n",
    "print(f\"AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giao dịch tương tự giữa train và validation: 0\n"
     ]
    }
   ],
   "source": [
    "similar_rows = pd.merge(new_train_data, new_val_data, on=['category', 'amt', 'trans_date_trans_time'])\n",
    "print(f\"Số lượng giao dịch tương tự giữa train và validation: {len(similar_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tỷ lệ giao dịch gian lận trong tập train: 0.0059\n",
      "Tỷ lệ giao dịch gian lận trong tập validation: 0.0054\n"
     ]
    }
   ],
   "source": [
    "fraud_ratio_train = new_train_data['is_fraud'].mean()\n",
    "fraud_ratio_val = new_val_data['is_fraud'].mean()\n",
    "print(f\"Tỷ lệ giao dịch gian lận trong tập train: {fraud_ratio_train:.4f}\")\n",
    "print(f\"Tỷ lệ giao dịch gian lận trong tập validation: {fraud_ratio_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "grocery_pos       1411\n",
      "shopping_net      1387\n",
      "misc_net           734\n",
      "shopping_pos       672\n",
      "gas_transport      502\n",
      "misc_pos           195\n",
      "kids_pets          193\n",
      "entertainment      188\n",
      "personal_care      173\n",
      "home               161\n",
      "food_dining        125\n",
      "grocery_net        108\n",
      "health_fitness     103\n",
      "travel              95\n",
      "Name: count, dtype: int64\n",
      "count    6047.000000\n",
      "mean      530.939836\n",
      "std       390.658590\n",
      "min         1.060000\n",
      "25%       246.090000\n",
      "50%       389.570000\n",
      "75%       901.190000\n",
      "max      1376.040000\n",
      "Name: amt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fraud_train_data = new_train_data[new_train_data['is_fraud'] == 1]\n",
    "print(fraud_train_data['category'].value_counts())  # Kiểm tra loại giao dịch gian lận phổ biến\n",
    "print(fraud_train_data['amt'].describe())  # Thống kê về số tiền giao dịch gian lận"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data.copy()\n",
    "\n",
    "        # Convert date to timestamp and extract time-based features\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time'])\n",
    "        self.data['hour_of_day'] = self.data['trans_date_trans_time'].dt.hour\n",
    "        self.data['day_of_week'] = self.data['trans_date_trans_time'].dt.dayofweek\n",
    "        self.data['trans_date_trans_time'] = self.data['trans_date_trans_time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category and merchant columns\n",
    "        self.category_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.category_encoder.fit_transform(self.data['category'])\n",
    "        self.merchant_encoder = LabelEncoder()\n",
    "        self.data['merchant'] = self.merchant_encoder.fit_transform(self.data['merchant'])\n",
    "\n",
    "        # Normalize amt\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Approximate distance between user location and merchant location (faster than geodesic)\n",
    "        self.data['distance_to_merchant'] = self.data.apply(\n",
    "            lambda row: sqrt((row['lat'] - row['merch_lat'])**2 + (row['long'] - row['merch_long'])**2), axis=1\n",
    "        )\n",
    "\n",
    "        # Fill NaNs for missing values\n",
    "        self.data.fillna(0, inplace=True)\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time', 'hour_of_day', 'day_of_week', 'distance_to_merchant']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, 2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[3] for s in seq], prepend=seq[0][3])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define the TH-LSTM model\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.dropout_hidden = nn.Dropout(p=0.4)\n",
    "        self.dropout_attention = nn.Dropout(p=0.3)\n",
    "        self.dropout_classifier = nn.Dropout(p=0.4)\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wfs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wis = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wuh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wux = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wus = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.WTh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.WTx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.WTs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bT = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wos = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)\n",
    "\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            f_t = torch.sigmoid(self.Wfh(h_prev) + self.Wfx(x_t) + self.Wfs(s_t) + self.bf)\n",
    "            i_t = torch.sigmoid(self.Wih(h_prev) + self.Wix(x_t) + self.Wis(s_t) + self.bi)\n",
    "            T_t = torch.sigmoid(self.WTh(h_prev) + self.WTx(x_t) + self.WTs(s_t) + self.bT)\n",
    "            zeta_t = torch.tanh(self.Wuh(h_prev) + self.Wux(x_t) + self.Wus(s_t) + self.bu)\n",
    "            c_t = f_t * c_prev + i_t * zeta_t + T_t * s_t\n",
    "            o_t = torch.sigmoid(self.Woh(h_prev) + self.Wox(x_t) + self.Wos(s_t) + self.bo)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            h_t = self.dropout_hidden(h_t)  # Dropout at the output layer\n",
    "\n",
    "            historical_states = torch.cat((historical_states[:, 1:], h_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        q_t = torch.cat((h_t, c_t), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = self.dropout_attention(torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1))\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        y_pred = torch.sigmoid(self.dropout_classifier(self.classifier(e_t)))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated input_dim: 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x3 and 4x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 206\u001b[0m\n\u001b[1;32m    204\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), y_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    205\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 206\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[1;32m    208\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 129\u001b[0m, in \u001b[0;36mTH_LSTM.forward\u001b[0;34m(self, X_seq)\u001b[0m\n\u001b[1;32m    126\u001b[0m x_t \u001b[38;5;241m=\u001b[39m X_seq[:, t, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    127\u001b[0m delta_t \u001b[38;5;241m=\u001b[39m X_seq[:, t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m s_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWsh(h_prev) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWsx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWst(delta_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs)\n\u001b[1;32m    130\u001b[0m f_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWfh(h_prev) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWfx(x_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWfs(s_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf)\n\u001b[1;32m    131\u001b[0m i_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWih(h_prev) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWix(x_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWis(s_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbi)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x3 and 4x32)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data.copy()\n",
    "\n",
    "        # Convert date to timestamp and extract time-based features\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time'])\n",
    "        self.data['hour_of_day'] = self.data['trans_date_trans_time'].dt.hour\n",
    "        self.data['day_of_week'] = self.data['trans_date_trans_time'].dt.dayofweek\n",
    "        self.data['trans_date_trans_time'] = self.data['trans_date_trans_time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category and merchant columns\n",
    "        self.category_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.category_encoder.fit_transform(self.data['category'])\n",
    "        self.merchant_encoder = LabelEncoder()\n",
    "        self.data['merchant'] = self.merchant_encoder.fit_transform(self.data['merchant'])\n",
    "\n",
    "        # Normalize amt\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Approximate distance between user location and merchant location\n",
    "        self.data['distance_to_merchant'] = self.data.apply(\n",
    "            lambda row: sqrt((row['lat'] - row['merch_lat'])**2 + (row['long'] - row['merch_long'])**2), axis=1\n",
    "        )\n",
    "\n",
    "        # Fill NaNs for missing values\n",
    "        self.data.fillna(0, inplace=True)\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'hour_of_day', 'day_of_week', 'distance_to_merchant', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, 5]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-2] for s in seq])  # Exclude fraud label and timestamp\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]\n",
    "        print(f\"x_seq shape: {x_seq.shape}\")  # Debug print for shape\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define the TH-LSTM model\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.dropout_hidden = nn.Dropout(p=0.4)\n",
    "        self.dropout_attention = nn.Dropout(p=0.3)\n",
    "        self.dropout_classifier = nn.Dropout(p=0.4)\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wfs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wis = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wuh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wux = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wus = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.WTh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.WTx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.WTs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bT = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wos = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)\n",
    "\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            f_t = torch.sigmoid(self.Wfh(h_prev) + self.Wfx(x_t) + self.Wfs(s_t) + self.bf)\n",
    "            i_t = torch.sigmoid(self.Wih(h_prev) + self.Wix(x_t) + self.Wis(s_t) + self.bi)\n",
    "            T_t = torch.sigmoid(self.WTh(h_prev) + self.WTx(x_t) + self.WTs(s_t) + self.bT)\n",
    "            zeta_t = torch.tanh(self.Wuh(h_prev) + self.Wux(x_t) + self.Wus(s_t) + self.bu)\n",
    "            c_t = f_t * c_prev + i_t * zeta_t + T_t * s_t\n",
    "            o_t = torch.sigmoid(self.Woh(h_prev) + self.Wox(x_t) + self.Wos(s_t) + self.bo)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            h_t = self.dropout_hidden(h_t)  # Dropout at the output layer\n",
    "\n",
    "            historical_states = torch.cat((historical_states[:, 1:], h_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        q_t = torch.cat((h_t, c_t), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = self.dropout_attention(torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1))\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        y_pred = torch.sigmoid(self.dropout_classifier(self.classifier(e_t)))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_true = []\n",
    "    y_pred_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred_prob.extend(y_pred.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(data_loader)\n",
    "    return val_loss, y_true, y_pred_prob\n",
    "\n",
    "\n",
    "# Training loop\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 8  # Update for added features\n",
    "    hidden_dim = 32\n",
    "    memory_size = 10\n",
    "    seq_len = 5\n",
    "    epochs = 10\n",
    "    patience = 2\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Get the correct input dimension\n",
    "    first_seq, _ = train_dataset[0]  # Get the first sequence from the dataset\n",
    "    input_dim = first_seq.shape[1]  # Number of features per timestep\n",
    "    print(f\"Updated input_dim: {input_dim}\")\n",
    "\n",
    "# Define the model with the correct input dimension\n",
    "\n",
    "    model = TH_LSTM(input_dim, hidden_dim, memory_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_pred_binary = (y_pred >= 0.5).float()\n",
    "            correct += (y_pred_binary == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        val_loss, y_true, y_pred_prob = evaluate_model(model, val_loader, criterion)\n",
    "        y_pred_val_binary = (np.array(y_pred_prob) >= 0.5).astype(int)\n",
    "        val_accuracy = accuracy_score(y_true, y_pred_val_binary)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] | Training Loss: {total_loss / len(train_loader):.4f} | Training Accuracy: {train_accuracy:.4f} | Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Loss: 14.9789\n",
      "Epoch [2/3] Loss: 1.7341\n",
      "Epoch [3/3] Loss: 10.7996\n",
      "(555719, 1)\n",
      "(555719, 1)\n",
      "Threshold: 0.10 | Accuracy: 0.9958 | Precision: 0.0981 | Recall: 0.0121 | F1: 0.0216 | AUC: 0.5058\n",
      "(555719, 1)\n",
      "(555719, 1)\n",
      "Threshold: 0.20 | Accuracy: 0.9962 | Precision: 1.0000 | Recall: 0.0112 | F1: 0.0221 | AUC: 0.5056\n",
      "(555719, 1)\n",
      "(555719, 1)\n",
      "Threshold: 0.30 | Accuracy: 0.9962 | Precision: 1.0000 | Recall: 0.0107 | F1: 0.0212 | AUC: 0.5054\n",
      "(555719, 1)\n",
      "(555719, 1)\n",
      "Threshold: 0.40 | Accuracy: 0.9962 | Precision: 1.0000 | Recall: 0.0103 | F1: 0.0203 | AUC: 0.5051\n",
      "(555719, 1)\n",
      "(555719, 1)\n",
      "Threshold: 0.50 | Accuracy: 0.9962 | Precision: 1.0000 | Recall: 0.0103 | F1: 0.0203 | AUC: 0.5051\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert date to timestamp\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time']).apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.label_encoder.fit_transform(self.data['category'])\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, -2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]  # Return only `x_seq` and `y_label`\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Simple Linear Model for Fraud Detection\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        # A single linear layer for binary classification\n",
    "        self.classifier = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        # Average over the sequence to create a single vector representation\n",
    "        X_avg = X_seq.mean(dim=1)  # Shape: [batch_size, input_dim]\n",
    "        y_pred = torch.sigmoid(self.classifier(X_avg))  # Sigmoid to get probabilities\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 4  # category, amt, delta_t\n",
    "    seq_len = 20\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTrain.csv\", seq_len=seq_len)\n",
    "    test_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTest.csv\", seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate model, optimizer, and loss function\n",
    "\n",
    "    model = SimpleLinearModel(input_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    y_true_test = []\n",
    "    y_pred_test_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            y_pred = model(X_batch)\n",
    "            y_true_test.extend(y_batch.cpu().numpy())\n",
    "            y_pred_test_prob.extend(y_pred.cpu().numpy())\n",
    "\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    for threshold in thresholds:\n",
    "        y_pred_test_binary = (np.array(y_pred_test_prob) >= threshold).astype(int)\n",
    "        y_true_test = np.array(y_true_test) if isinstance(y_true_test, list) else y_true_test\n",
    "        print(y_true_test.shape)  # Print first 10 true labels\n",
    "        print(y_pred_test_binary.shape)  # Print first 10 predicted labels\n",
    "        accuracy = accuracy_score(y_true_test, y_pred_test_binary)\n",
    "        precision = precision_score(y_true_test, y_pred_test_binary)\n",
    "        recall = recall_score(y_true_test, y_pred_test_binary)\n",
    "        f1 = f1_score(y_true_test, y_pred_test_binary)\n",
    "        auc = roc_auc_score(y_true_test, y_pred_test_binary)\n",
    "\n",
    "        print(f\"Threshold: {threshold:.2f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "Dropout layers for regularization (reduce overfitting).\n",
    "L2 regularization (weight decay) in the optimizer.\n",
    "Weighted loss function for handling class imbalance.\n",
    "Added a validation split to monitor performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Loss: 0.7294\n",
      "Epoch [2/3] Loss: 0.7293\n",
      "Epoch [3/3] Loss: 0.7293\n",
      "Threshold: 0.10 | Accuracy: 0.9958 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | AUC: 0.4998\n",
      "Threshold: 0.20 | Accuracy: 0.9960 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | AUC: 0.4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducanh/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.30 | Accuracy: 0.9961 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducanh/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.40 | Accuracy: 0.9961 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducanh/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.50 | Accuracy: 0.9961 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert date to timestamp\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time']).apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.label_encoder.fit_transform(self.data['category'])\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, -2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]  # Return only `x_seq` and `y_label` (no separate `time_intervals`)\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# TH-LSTM Model (with time-aware gate `T_t`)\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)  # For time intervals\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout to gates\n",
    "\n",
    "\n",
    "        self.Wfh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wfx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wfs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wih = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wix = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wis = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wuh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wux = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wus = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.WTh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.WTx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.WTs = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bT = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Woh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wox = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wos = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]  # Features excluding delta_t\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)  # Time interval (delta_t)\n",
    "\n",
    "            # Time-aware state\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            s_t = self.dropout(s_t)\n",
    "\n",
    "            # Gates\n",
    "            f_t = torch.sigmoid(self.Wfh(h_prev) + self.Wfx(x_t) + self.Wfs(s_t) + self.bf)\n",
    "            i_t = torch.sigmoid(self.Wih(h_prev) + self.Wix(x_t) + self.Wis(s_t) + self.bi)\n",
    "            T_t = torch.sigmoid(self.WTh(h_prev) + self.WTx(x_t) + self.WTs(s_t) + self.bT)  # Time-aware gate\n",
    "\n",
    "            # Candidate cell state\n",
    "            zeta_t = torch.tanh(self.Wuh(h_prev) + self.Wux(x_t) + self.Wus(s_t) + self.bu)\n",
    "\n",
    "            # New cell state\n",
    "            c_t = f_t * c_prev + i_t * zeta_t + T_t * s_t\n",
    "\n",
    "            # Hidden state\n",
    "            o_t = torch.sigmoid(self.Woh(h_prev) + self.Wox(x_t) + self.Wos(s_t) + self.bo)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            # Save historical hidden states for attention\n",
    "            historical_states = torch.cat((historical_states[:, 1:], h_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = h_t, c_t\n",
    "\n",
    "        # Attention mechanism\n",
    "        q_t = torch.cat((h_t, c_t), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1)\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        # Final classification\n",
    "        y_pred = torch.sigmoid(self.classifier(e_t))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Training and testing\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 3  # category, amt, delta_t\n",
    "    hidden_dim = 64\n",
    "    memory_size = 10\n",
    "    seq_len = 5\n",
    "\n",
    "    train_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTrain.csv\", seq_len=seq_len)\n",
    "    test_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTest.csv\", seq_len=seq_len)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TH_LSTM(input_dim, hidden_dim, memory_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Add L2 regularization\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0]).to('cuda'))  # Weighted loss for imbalance\n",
    "\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:  # Unpack 2 values\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    y_true_test = []\n",
    "    y_pred_test_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:  # Unpack 2 values\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            y_pred = model(X_batch)\n",
    "            y_true_test.extend(y_batch.cpu().numpy())\n",
    "            y_pred_test_prob.extend(y_pred.cpu().numpy())\n",
    "\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    for threshold in thresholds:\n",
    "        y_pred_test_binary = (np.array(y_pred_test_prob) >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_true_test, y_pred_test_binary)\n",
    "        precision = precision_score(y_true_test, y_pred_test_binary)\n",
    "        recall = recall_score(y_true_test, y_pred_test_binary)\n",
    "        f1 = f1_score(y_true_test, y_pred_test_binary)\n",
    "        auc = roc_auc_score(y_true_test, y_pred_test_binary)\n",
    "\n",
    "        print(f\"Threshold: {threshold:.2f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Train Loss: 0.1272 | Val Loss: 0.1188\n",
      "Epoch [2/3] Train Loss: 0.1127 | Val Loss: 0.1188\n",
      "Epoch [3/3] Train Loss: 0.1127 | Val Loss: 0.1219\n",
      "Threshold: 0.10 | Accuracy: 0.9972 | Precision: 0.6317 | Recall: 0.6373 | F1: 0.6345 | AUC: 0.8179\n",
      "Threshold: 0.20 | Accuracy: 0.9972 | Precision: 0.6317 | Recall: 0.6373 | F1: 0.6345 | AUC: 0.8179\n",
      "Threshold: 0.30 | Accuracy: 0.9972 | Precision: 0.6317 | Recall: 0.6373 | F1: 0.6345 | AUC: 0.8179\n",
      "Threshold: 0.40 | Accuracy: 0.9972 | Precision: 0.6317 | Recall: 0.6373 | F1: 0.6345 | AUC: 0.8179\n",
      "Threshold: 0.50 | Accuracy: 0.9972 | Precision: 0.6317 | Recall: 0.6373 | F1: 0.6345 | AUC: 0.8179\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Dataset class for grouped sequences of transactions by `cc_num`\n",
    "class CreditCardFraudDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert date to timestamp\n",
    "        self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time']).apply(lambda x: x.timestamp())\n",
    "\n",
    "        # Encode category column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['category'] = self.label_encoder.fit_transform(self.data['category'])\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[['amt']] = scaler.fit_transform(self.data[['amt']])\n",
    "\n",
    "        # Group transactions by `cc_num` and create sequences\n",
    "        self.seq_len = seq_len\n",
    "        self.sequences = []\n",
    "        grouped = self.data.groupby('cc_num')\n",
    "        for _, group in grouped:\n",
    "            group = group[['category', 'amt', 'is_fraud', 'trans_date_trans_time']].values\n",
    "            for i in range(len(group)):\n",
    "                if i < self.seq_len - 1:\n",
    "                    padding = [group[0]] * (self.seq_len - i - 1)\n",
    "                    seq = padding + group[:i + 1].tolist()\n",
    "                else:\n",
    "                    seq = group[i - self.seq_len + 1:i + 1].tolist()\n",
    "\n",
    "                label = group[i, -2]  # Fraud label of the current transaction\n",
    "                time_intervals = np.diff([s[-1] for s in seq], prepend=seq[0][-1])  # Time differences\n",
    "                time_intervals = time_intervals.reshape(-1, 1)  # Reshape for compatibility\n",
    "                seq_features = np.array([s[:-1] for s in seq])  # Remove timestamp from features\n",
    "                seq_features = np.concatenate((seq_features, time_intervals), axis=1)  # Add time intervals as feature\n",
    "                self.sequences.append((seq_features, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq, y_label = self.sequences[idx]\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# TH-LSTM Model (with time-aware gate `T_t`)\n",
    "class TH_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_size):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # LSTM weights\n",
    "        self.Wsh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wsx = nn.Linear(input_dim, hidden_dim)\n",
    "        self.Wst = nn.Linear(1, hidden_dim)  # For time intervals\n",
    "        self.bs = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        # Attention module\n",
    "        self.Waq = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.Wah = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ba = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.vt = nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "\n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, X_seq):\n",
    "        batch_size = X_seq.size(0)\n",
    "        seq_len = X_seq.size(1)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(X_seq.device)\n",
    "        historical_states = torch.zeros(batch_size, self.memory_size, self.hidden_dim).to(X_seq.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = X_seq[:, t, :-1]  # Features excluding delta_t\n",
    "            delta_t = X_seq[:, t, -1].view(-1, 1)  # Time interval (delta_t)\n",
    "            s_t = torch.tanh(self.Wsh(h_prev) + self.Wsx(x_t) + self.Wst(delta_t) + self.bs)\n",
    "            s_t = self.dropout(s_t)\n",
    "\n",
    "            # Save historical hidden states for attention\n",
    "            historical_states = torch.cat((historical_states[:, 1:], s_t.unsqueeze(1)), dim=1)\n",
    "            h_prev, c_prev = s_t, s_t\n",
    "\n",
    "        # Attention mechanism\n",
    "        q_t = torch.cat((h_prev, c_prev), dim=1)\n",
    "        o_t_i = torch.tanh(self.Waq(q_t).unsqueeze(1) + self.Wah(historical_states))\n",
    "        alpha_t_i = torch.exp(torch.matmul(o_t_i, self.vt)).squeeze(-1)\n",
    "        alpha_t_i = alpha_t_i / torch.sum(alpha_t_i, dim=1, keepdim=True)\n",
    "        e_t = torch.sum(alpha_t_i.unsqueeze(-1) * historical_states, dim=1)\n",
    "\n",
    "        # Final classification\n",
    "        y_pred = self.classifier(e_t)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Training, validation, and testing\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    input_dim = 3  # category, amt, delta_t\n",
    "    hidden_dim = 64\n",
    "    memory_size = 10\n",
    "    seq_len = 5\n",
    "\n",
    "    train_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTrain.csv\", seq_len=seq_len)\n",
    "    test_dataset = CreditCardFraudDataset(\"/home/ducanh/Credit Card Transactions Fraud Detection/fraudTest.csv\", seq_len=seq_len)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TH_LSTM(input_dim, hidden_dim, memory_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0]).to('cuda'))\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5  # Early stopping patience\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += criterion(y_pred, y_batch).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Train Loss: {total_loss / len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    # **Testing and Evaluation Phase**\n",
    "    model.eval()\n",
    "    y_true_test = []\n",
    "    y_pred_test_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to('cuda'), y_batch.view(-1, 1).to('cuda')\n",
    "            y_pred = model(X_batch)\n",
    "            y_true_test.extend(y_batch.cpu().numpy())\n",
    "            y_pred_test_prob.extend(y_pred.cpu().numpy())\n",
    "\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    for threshold in thresholds:\n",
    "        y_pred_test_binary = (np.array(y_pred_test_prob) >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_true_test, y_pred_test_binary)\n",
    "        precision = precision_score(y_true_test, y_pred_test_binary)\n",
    "        recall = recall_score(y_true_test, y_pred_test_binary)\n",
    "        f1 = f1_score(y_true_test, y_pred_test_binary)\n",
    "        auc = roc_auc_score(y_true_test, y_pred_test_binary)\n",
    "\n",
    "        print(f\"Threshold: {threshold:.2f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ducanh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
