{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3208497/3117668709.py:25: FutureWarning: The provided callable <function mean at 0x707b009d9f30> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  age_piv_2 = pd.pivot_table(data=df,\n",
      "/tmp/ipykernel_3208497/3117668709.py:38: FutureWarning: The provided callable <function mean at 0x707b009d9f30> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  job_txn_piv_2 = pd.pivot_table(data=df,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1241103, 19)\n",
      "Test shape: (611291, 19)\n",
      "Shape of training data: ((1241103, 17), (1241103,))\n",
      "Shape of testing data: ((611291, 17), (611291,))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'/home/ducanh/Credit Card Transactions Fraud Detection/Datasets/combined_data.csv')\n",
    "\n",
    "# Xử lý thời gian\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['trans_date_trans_time_numeric'] = df['trans_date_trans_time'].apply(lambda x: x.timestamp())\n",
    "df['trans_hour'] = df['trans_date_trans_time'].dt.time.apply(lambda x: str(x)[:2])\n",
    "\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "df['cust_age'] = df['dob'].dt.year.apply(lambda x: 2021 - x)\n",
    "df['cust_age_groups'] = df['cust_age'].apply(lambda x: 'below 10' if x < 10 else ('10-20' if x >= 10 and x < 20 else ('20-30' if x >= 20 and x < 30 else ('30-40' if x >= 30 and x < 40 else ('40-50' if x >= 40 and x < 50 else ('50-60' if x >= 50 and x < 60 else ('60-70' if x >= 60 and x < 70 else ('70-80' if x >= 70 and x < 80 else 'Above 80'))))))))\n",
    "\n",
    "age_piv_2 = pd.pivot_table(data=df,\n",
    "                           index='cust_age_groups',\n",
    "                           columns='is_fraud',\n",
    "                           values='amt',\n",
    "                           aggfunc=np.mean)\n",
    "age_piv_2.sort_values(by=1, ascending=True, inplace=True)\n",
    "age_dic = {k: v for (k, v) in zip(age_piv_2.index.values, age_piv_2.reset_index().index.values)}\n",
    "df['cust_age_groups'] = df['cust_age_groups'].map(age_dic)\n",
    "\n",
    "merch_cat = df[df['is_fraud'] == 1].groupby('category')['amt'].mean().sort_values(ascending=True)\n",
    "merch_cat_dic = {k: v for (k, v) in zip(merch_cat.index.values, merch_cat.reset_index().index.values)}\n",
    "df['category'] = df['category'].map(merch_cat_dic)\n",
    "\n",
    "job_txn_piv_2 = pd.pivot_table(data=df,\n",
    "                               index='job',\n",
    "                               columns='is_fraud',\n",
    "                               values='amt',\n",
    "                               aggfunc=np.mean)\n",
    "job_cat_dic = {k: v for (k, v) in zip(job_txn_piv_2.index.values, job_txn_piv_2.reset_index().index.values)}\n",
    "df['job'] = df['job'].map(job_cat_dic)\n",
    "\n",
    "df['merchant_num'] = pd.factorize(df['merchant'])[0]\n",
    "df['last_num'] = pd.factorize(df['last'])[0]\n",
    "df['street_num'] = pd.factorize(df['street'])[0]\n",
    "df['city_num'] = pd.factorize(df['city'])[0]\n",
    "df['zip_num'] = pd.factorize(df['zip'])[0]\n",
    "df['state_num'] = pd.factorize(df['state'])[0]\n",
    "\n",
    "df = pd.get_dummies(data=df, columns=['gender'], drop_first=True, dtype='int')\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'trans_date_trans_time', 'merchant', 'first', 'last', 'street', 'city', 'state', 'lat', 'long', 'dob',\n",
    "             'unix_time', 'merch_lat', 'merch_long', 'city_pop']\n",
    "df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "# 2️⃣ Train_test_split\n",
    "# =============================================================================\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.33, random_state=42, stratify=df['is_fraud'])\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# Drop cột trans_num từ cả train và test\n",
    "train.drop('trans_num', axis=1, inplace=True)\n",
    "test.drop('trans_num', axis=1, inplace=True)\n",
    "\n",
    "# Tách features và label\n",
    "y_train = train['is_fraud']\n",
    "X_train = train.drop('is_fraud', axis=1)\n",
    "\n",
    "y_test = test['is_fraud']\n",
    "X_test = test.drop('is_fraud', axis=1)\n",
    "\n",
    "print('Shape of training data:', (X_train.shape, y_train.shape))\n",
    "print('Shape of testing data:', (X_test.shape, y_test.shape))\n",
    "\n",
    "# =============================================================================\n",
    "# 3️⃣ Scaling dữ liệu\n",
    "# =============================================================================\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)\n",
    "\n",
    "# Convert lại thành DataFrame\n",
    "X_train_sc = pd.DataFrame(data=X_train_sc, columns=X_train.columns)\n",
    "X_test_sc = pd.DataFrame(data=X_test_sc, columns=X_test.columns)\n",
    "\n",
    "def create_sequences_transactional_expansion(df, memory_size):\n",
    "    sequences, labels = [], []\n",
    "    \n",
    "    # Nhóm theo 'cc_num' (số thẻ tín dụng của người dùng)\n",
    "    grouped = df.groupby('cc_num')\n",
    "    \n",
    "    for user_id, group in grouped:\n",
    "        # Sắp xếp theo thời gian (đảm bảo rằng trans_date_trans_time đã là timestamp)\n",
    "        group = group.sort_values(by='trans_date_trans_time_numeric')\n",
    "        \n",
    "        # Lấy các giá trị (loại bỏ 'is_fraud' và 'cc_num' vì đây là features)\n",
    "        values = group.drop(columns=['is_fraud', 'cc_num']).values\n",
    "        targets = group['is_fraud'].values\n",
    "        \n",
    "        n = len(group)\n",
    "        \n",
    "        # Tạo chuỗi giao dịch cho mỗi giao dịch\n",
    "        for i in range(n):\n",
    "            if i < memory_size:\n",
    "                # Nếu số giao dịch hiện tại ít hơn 'memory_size', sao chép giao dịch đầu tiên\n",
    "                pad_needed = memory_size - (i + 1)\n",
    "                # Sao chép giao dịch đầu tiên cho đủ số lượng pad\n",
    "                pad = np.repeat(values[0:1, :], pad_needed, axis=0)\n",
    "                seq = np.concatenate((pad, values[:i+1]), axis=0)\n",
    "            else:\n",
    "                # Nếu đủ giao dịch, lấy sequence gồm các giao dịch từ (i - memory_size + 1) đến i\n",
    "                seq = values[i-memory_size+1:i+1]\n",
    "            \n",
    "            # Thêm sequence và label vào danh sách\n",
    "            sequences.append(seq)\n",
    "            labels.append(targets[i])\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "memory_size = 600  # Chọn memory_size như một giá trị cố định (hoặc có thể thử các giá trị khác như 30, 50)\n",
    "train_seq_df = X_train_sc.copy()\n",
    "train_seq_df['is_fraud'] = y_train.values\n",
    "\n",
    "test_seq_df = X_test_sc.copy()\n",
    "test_seq_df['is_fraud'] = y_test.values\n",
    "\n",
    "# Tạo các chuỗi theo phương pháp Transactional Expansion\n",
    "X_train_seq, y_train_seq = create_sequences_transactional_expansion(train_seq_df, memory_size)\n",
    "X_test_seq, y_test_seq = create_sequences_transactional_expansion(test_seq_df, memory_size)\n",
    "\n",
    "print(\"Sequence shape (train):\", X_train_seq.shape)\n",
    "print(\"Sequence shape (test):\", X_test_seq.shape)\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # shape: (num_sequences, sequence_length, num_features)\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = FraudDataset(torch.tensor(X_train_seq, dtype=torch.float32),\n",
    "                               torch.tensor(y_train_seq, dtype=torch.float32))\n",
    "test_dataset = FraudDataset(torch.tensor(X_test_seq, dtype=torch.float32),\n",
    "                              torch.tensor(y_test_seq, dtype=torch.float32))\n",
    "\n",
    "# Shuffle chỉ xáo trộn thứ tự các sequence, không làm xáo trộn bên trong mỗi sequence\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Xây dựng mô hình GRU\n",
    "# =============================================================================\n",
    "class FraudLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(FraudLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, sequence_length, features]\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        # Use the hidden state from the last LSTM layer at the last time step\n",
    "        out = self.fc(h_n[-1])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# 5️⃣ Evaluation Function\n",
    "def evaluate_model(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze().cpu().numpy()\n",
    "            all_preds.extend(outputs)\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Compute ROC AUC score\n",
    "    auc = roc_auc_score(all_targets, all_preds)\n",
    "    \n",
    "    # Search for the best threshold (using thresholds 0.1 to 0.9) based on F1 score\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for t in thresholds:\n",
    "        binary_preds = (all_preds > t).astype(int)\n",
    "        f1 = f1_score(all_targets, binary_preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    combined_metric = (best_f1 + auc) / 2\n",
    "    \n",
    "    # Also compute additional metrics using the best threshold\n",
    "    binary_preds = (all_preds > best_threshold).astype(int)\n",
    "    cm = confusion_matrix(all_targets, binary_preds)\n",
    "    TP = cm[1, 1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0\n",
    "    FP = cm[0, 1] if cm.shape[1] > 1 else 0\n",
    "    FN = cm[1, 0] if cm.shape[0] > 1 else 0\n",
    "    TN = cm[0, 0]\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    return best_threshold, best_f1, auc, combined_metric, accuracy, precision, recall\n",
    "\n",
    "# 6️⃣ Training Function without checkpoint saving\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
    "    best_loss = float('inf')\n",
    "    best_combined_metric_test = -float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Biến lưu kết quả tốt nhất\n",
    "    best_epoch = None\n",
    "    best_train_metrics = None\n",
    "    best_test_metrics = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f'\\nEpoch {epoch+1}, Loss: {average_loss:.4f}')\n",
    "        \n",
    "        # Evaluate on the training set\n",
    "        train_threshold, train_f1, train_auc, train_combined, train_acc, train_prec, train_rec = evaluate_model(train_loader, model, device)\n",
    "        print(f\"Train Metrics - Best Threshold: {train_threshold:.2f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}, Combined: {train_combined:.4f}, Accuracy: {train_acc:.4f}, Precision: {train_prec:.4f}, Recall: {train_rec:.4f}\")\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_threshold, test_f1, test_auc, test_combined, test_acc, test_prec, test_rec = evaluate_model(test_loader, model, device)\n",
    "        print(f\"Test Metrics  - Best Threshold: {test_threshold:.2f}, F1: {test_f1:.4f}, AUC: {test_auc:.4f}, Combined: {test_combined:.4f}, Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}\")\n",
    "        \n",
    "        # Cập nhật thông tin của epoch tốt nhất dựa trên test_combined\n",
    "        if test_combined > best_combined_metric_test:\n",
    "            best_combined_metric_test = test_combined\n",
    "            best_epoch = epoch + 1\n",
    "            best_train_metrics = (train_f1, train_auc, train_combined)\n",
    "            best_test_metrics = (test_f1, test_auc, test_combined)\n",
    "            print(f'*** Best metrics updated at epoch {epoch+1} ***')\n",
    "        \n",
    "        # Early stopping: nếu loss không cải thiện trong 8 epoch liên tiếp\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= 8:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                break\n",
    "    # In ra kết quả tốt nhất của epoch đã chạy\n",
    "    print(\"\\n========== Final Best Results ==========\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Train Metrics - F1: {best_train_metrics[0]:.4f}, AUC: {best_train_metrics[1]:.4f}, Combined: {best_train_metrics[2]:.4f}\")\n",
    "    print(f\"Test Metrics  - F1: {best_test_metrics[0]:.4f}, AUC: {best_test_metrics[1]:.4f}, Combined: {best_test_metrics[2]:.4f}\")\n",
    "\n",
    "# 7️⃣ Model Initialization and Training\n",
    "# Model Initialization and Training\n",
    "input_size = X_train_seq.shape[2]  # number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "model = FraudLSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs for training.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 30\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
